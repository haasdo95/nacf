#+OPTIONS: ':t *:t -:t ::t <:t H:5 \n:nil ^:{} arch:headline author:t
#+OPTIONS: broken-links:nil c:nil creator:nil d:(not "LOGBOOK") date:t e:t
#+OPTIONS: email:nil f:t inline:t num:nil p:t pri:nil prop:nil stat:t tags:t
#+OPTIONS: tasks:t tex:t timestamp:t title:t toc:t todo:t |:t
#+TITLE: README
#+DATE: <2018-12-23 Sun>
#+AUTHOR: Nasy
#+EMAIL: nasyxx@gmail.com
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 27.0.50 (Org mode 9.1.9)

#+SETUPFILE: https://fniessen.github.io/org-html-themes/setup/theme-readtheorg.setup

* Prologue

Never had such a pure crawler like this ~nacf~.

Although I often write crawlers, I don't like to use huge frameworks, such as scrapy, but prefer
simple requests+bs4 or more general requests_html. But these two are inconvenient for a
crawler. Places, such as error retry or parallel crawling, need to be handwritten by myself. It is
not very difficult to write it, however writing too much can be tedious. Hence I started writing
this nacf (Nasy Crawler Framework), hoping to simplify some retry or parallel writing.

* Packages

#+caption: Packages
| Package       | Version | Description              |
|---------------+---------+--------------------------|
| requests      |  2.21.0 | Python HTTP for Humans.  |
| requests-html |   0.9.0 | HTML Parsing for Humans. |

* Development Process

#+include: "todo.org" :minlevel 2

* Epoligue

** History

#+include: "CHANGELOG" :minlevel 3
